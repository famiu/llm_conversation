"""Module for managing a conversation between AI agents."""

from collections.abc import Iterator
from dataclasses import dataclass, field
from pathlib import Path
from typing import TypedDict

from .ai_agent import AIAgent


@dataclass
class ConversationManager:
    """Manager for a conversation between AI agents."""

    class _ConversationLogItem(TypedDict):
        agent: str
        content: str

    agents: list[AIAgent]
    initial_message: str | None
    use_markdown: bool = False
    allow_termination: bool = False
    _conversation_log: list[_ConversationLogItem] = field(default_factory=list, init=False)
    _original_system_prompts: list[str] = field(init=False)

    def __post_init__(self) -> None:  # noqa: D105
        self._original_system_prompts = [agent.system_prompt for agent in self.agents]

        # Modify system prompt to include termination instructions if allowed
        additional_instructions: str = ""

        if self.use_markdown:
            additional_instructions += (
                "You may use Markdown for text formatting. "
                "Examples: *italic*, **bold**, `code`, [link](https://example.com), etc.\n\n"
            )

        # TODO: Use structured output instead of <TERMINATE> token.
        if self.allow_termination:
            additional_instructions += (
                "You may terminate the conversation with the `<TERMINATE>` token "
                "if you believe it has reached a natural conclusion. "
                "Do not include the token in your message otherwise.\n\n"
            )

        # Updated system prompts for each agent. This gives the agents more context about the conversation and their
        # role. It also makes the agents use dialogue markers in their responses, which helps other AIs better interpret
        # the conversation.
        #
        # This dialogue marker is stripped from the actual output stream generated by run_conversation(), but other AI
        # agents can still see it in the conversation history, and it helps them generate more coherent responses.
        for agent in self.agents:
            other_agents = ", ".join([a.name for a in self.agents if a != agent])
            agent.system_prompt = (
                "This is a conversation between AI agents.\n\n"
                + f"You are named {agent.name}. The other agents are {other_agents}. "
                + "Your task is to play the role you're given and continue the conversation.\n\n"
                + f"Always start your message with your name followed by a colon (e.g., '{agent.name}: Hello')\n\n"
                + f"This is the prompt for your role: {agent.system_prompt}\n\n"
                + additional_instructions
            ).strip()

    # TODO: Support JSON output.
    def save_conversation(self, filename: Path) -> None:
        """Save the conversation log to a file.

        Args:
            filename (Path): Path to save the conversation log to
        """
        with open(filename, "w", encoding="utf-8") as f:
            for i, agent in enumerate(self.agents, start=1):
                _ = f.write(f"=== Agent {i} ===\n\n")
                _ = f.write(f"Name: {agent.name}\n")
                _ = f.write(f"Model: {agent.model}\n")
                _ = f.write(f"Temperature: {agent.temperature}\n")
                _ = f.write(f"Context Size: {agent.ctx_size}\n")
                _ = f.write(f"System Prompt: {self._original_system_prompts[i - 1]}\n\n")

            _ = f.write("=== Conversation ===\n\n")

            for i, msg in enumerate(self._conversation_log):
                if i > 0:
                    _ = f.write("\n" + "\u2500" * 80 + "\n\n")

                _ = f.write(f"{msg['agent']}: {msg['content']}\n")

    def run_conversation(self) -> Iterator[tuple[str, Iterator[str]]]:
        """Generate an iterator of conversation responses.

        Yields:
            (str, Iterator[str]): A tuple of the agent name and an iterator of response chunks.
        """
        full_message = self.initial_message
        turn_count = 0

        def add_agent_message(agent_idx: int, message: str) -> None:
            """Add a message from an agent to the conversation log and the agents' message history.

            Args:
                agent_idx (int): Index of the agent the message is from
                message (str): Message content
            """
            for i, agent in enumerate(self.agents):
                agent.add_message(
                    self.agents[agent_idx].name,
                    "assistant" if i == agent_idx else "user",
                    # The dialogue marker got stripped from the message, so add it back here.
                    # This incentivizes the AI Agents to respond with the dialogue marker.
                    f"{self.agents[agent_idx].name}: {message}",
                )

            self._conversation_log.append({"agent": self.agents[agent_idx].name, "content": message})

        # If a non-empty initial message is provided, start with it.
        if self.initial_message is not None:
            # Make the first agent the one to say the initial message, and the second agent the one to respond.
            add_agent_message(0, self.initial_message)
            yield (self.agents[0].name, iter([self.initial_message]))
            turn_count += 1

        while True:
            agent_idx = turn_count % len(self.agents)
            current_agent = self.agents[agent_idx]
            response_stream = current_agent.get_response()
            message_chunks: list[str] = []

            def stream_chunks() -> Iterator[str]:
                # Strip the dialogue marker from the message.
                # Accumulate chunks until the full dialogue marker is found, then remove it and yield what's left of the
                # accumulated message. Continue yielding the rest of the chunks as they arrive.
                accumulated_message = ""
                while True:
                    chunk = next(response_stream)
                    accumulated_message += chunk

                    if accumulated_message.startswith(f"{current_agent.name}: "):
                        chunk = accumulated_message[len(current_agent.name) + 2 :]
                        message_chunks.append(chunk)
                        yield chunk
                        break

                for chunk in response_stream:
                    message_chunks.append(chunk)
                    yield chunk

            yield (current_agent.name, stream_chunks())

            full_message = "".join(message_chunks).strip()
            add_agent_message(agent_idx, full_message)

            # Check for termination token.
            # TODO: Filter out the `<TERMINATE>` token from the output to prevent it from displaying.
            if self.allow_termination and "<TERMINATE>" in full_message:
                break

            turn_count += 1
